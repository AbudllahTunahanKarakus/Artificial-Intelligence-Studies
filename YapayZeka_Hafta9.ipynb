{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjbWckn21QBh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dosya_yolu=\"/content/data (1).csv\"\n",
        "data=pd.read_csv(dosya_yolu)\n",
        "\n",
        "#ÖN İŞLEME\n",
        "data.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\n",
        "label_encoder=LabelEncoder()\n",
        "data[\"diagnosis\"]=label_encoder.fit_transform(data[\"diagnosis\"])\n",
        "X=data.drop(\"diagnosis\",axis=1)\n",
        "y=data[\"diagnosis\"]\n",
        "sc=StandardScaler()\n",
        "X_scaled=sc.fit_transform(X)\n",
        "X_train,X_test,y_train,y_test=train_test_split(X_scaled,y,test_size=0.3,random_state=42,stratify=y)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# --- HİPERPARAMETRE OPTİMİZASYONU (GRID SEARCH) ---\n",
        "svc=SVC()# Boş bir SVM modeli oluşturuyoruz.\n",
        "# C: Hata toleransı. Küçük C = Çok hata yapar ama geneli iyi görür (Soft Margin).\n",
        "# Büyük C = Hata yapmaktan korkar, veriyi ezberlemeye çalışır (Hard Margin).\n",
        "param_grid={\n",
        "    \"C\":[0.1,1,10,100,1000],\n",
        "    # kernel: Veriyi nasıl böleceği. Düz çizgi mi (linear), eğri mi (rbf), polinom mu (poly)?\n",
        "    \"kernel\":[\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n",
        "    # degree: Polinomun derecesi (x^2, x^3...). Sadece 'poly' kernel için geçerlidir.\n",
        "    'degree':[2,3,4],\n",
        "    # gamma: Bir noktanın ne kadar uzağı etkileyeceği.\n",
        "    # 'scale' ve 'auto' otomatik ayarlardır. Sayısal değer de verilebilir.\n",
        "    \"gamma\":[\"scale\",\"auto\"],\n",
        "    # coef0: Polinom ve Sigmoid kernel için serbest terim katsayısıdır. Modeli esnetir.\n",
        "    \"coef0\":[0.0,0.1,0.5]\n",
        "}\n",
        "# GridSearchCV Kurulumu\n",
        "# estimator=svc: Optimize edilecek model.\n",
        "# param_grid: Denenecek ayarlar.\n",
        "# cv=10: 10 Katlı çapraz doğrulama (Her ayarı 10 kez test et, sağlam olsun).\n",
        "# n_jobs=-1: Bilgisayardaki TÜM işlemcileri kullan (Hız için çok önemli).\n",
        "# verbose=1: İşlem sürerken ekrana bilgi bas.\n",
        "grid_search=GridSearchCV(estimator=svc,\n",
        "                         param_grid=param_grid,\n",
        "                         scoring=\"accuracy\",cv=10,n_jobs=-1,verbose=1)\n",
        "# --- ARAMAYI BAŞLAT ---\n",
        "# Bu işlem, yukarıdaki tüm kombinasyonları tek tek dener. Uzun sürebilir!\n",
        "grid_search.fit(X_train,y_train)\n",
        "# --- SONUÇLAR ---\n",
        "# En iyi sonucu veren modeli ve ayarları alıyoruz.\n",
        "best_model=grid_search.best_estimator_\n",
        "best_params=grid_search.best_params_\n",
        "print(\"En iyi parametreler:\",best_params)\n",
        "\n",
        "y_pred=best_model.predict(X_test)\n",
        "accuracy=best_model.score(X_test,y_test)\n",
        "print(\"En iyi modelin doğruluğu:\",accuracy)\n",
        "\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "cr=classification_report(y_test,y_pred,\n",
        "                         target_names=[\"Benign\",\"Malignant\"])\n",
        "print(\"Sınıflandırma Raporu:\\n\",cr)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm,annot=True,fmt=\"d\",cmap=\"Blues\",xticklabels=[\"Benign\",\"Malignant\"],yticklabels=[\"Benign\",\"Malignant\"])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyhJiyG0OLeg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import SelectKBest,f_classif\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "dosya_yolu=\"/content/data (1).csv\"\n",
        "data=pd.read_csv(dosya_yolu)\n",
        "\n",
        "#ÖN İŞLEME\n",
        "data.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\n",
        "label_encoder=LabelEncoder()\n",
        "data[\"diagnosis\"]=label_encoder.fit_transform(data[\"diagnosis\"])\n",
        "X=data.drop(\"diagnosis\",axis=1)\n",
        "y=data[\"diagnosis\"]\n",
        "sc=StandardScaler()\n",
        "X_scaled=sc.fit_transform(X)\n",
        "# --- SMOTE (Synthetic Minority Over-sampling Technique) ---\n",
        "# Azınlık sınıfı (Kanserli olanlar azsa) yapay verilerle çoğaltarak dengeyi sağlar.\n",
        "smote=SMOTE(random_state=42)\n",
        "# AÇIKLAMA (Senin Yorum Satırların):\n",
        "# smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
        "# ANLAMI: \"Azınlık sınıfını (1), Çoğunluk sınıfının (0) yarısı kadar olana dek çoğalt.\"\n",
        "# Örn: 300 tane 0 varsa, 1'leri artırıp 150 tane yapana kadar üretir.\n",
        "\n",
        "# smote = SMOTE(sampling_strategy={1: 300}, random_state=42)\n",
        "# ANLAMI: \"1 numaralı sınıftan (Malignant) elimde tam 300 tane olana kadar üret.\"\n",
        "# Bu manuel bir ayardır, sayıya direkt müdahale edersin.\n",
        "\n",
        "#smote=SMOTE(sampling_strategy=0.5,random_state=42)\n",
        "#smote=SMOTE(sampling_strategy={1: 300},random_state=42)\n",
        "\n",
        "# fit_resample: Veriyi alıp, sentetik verileri ekleyip bize artırılmış (augmented) X ve y döndürür.\n",
        "X_augmented,y_augmented=smote.fit_resample(X_scaled,y)\n",
        "\n",
        "print(y.value_counts())\n",
        "print(y_augmented.value_counts())\n",
        "# --- ÖZELLİK SEÇİMİ (FEATURE SELECTION) ---\n",
        "# SelectKBest: En iyi K tane özelliği seçer.\n",
        "# score_func=f_classif: Özelliklerin hedefle ilişkisini ölçen istatistiksel test (ANOVA).\n",
        "# k=10: 30 özellikten en etkili 10 tanesini seç diyoruz.\n",
        "k_best_selector=SelectKBest(score_func=f_classif,k=10)\n",
        "X_selected=k_best_selector.fit_transform(X_augmented,y_augmented)\n",
        "\n",
        "#FİX :APPLY GET_SUPPORT() TO THE ORGİNAL FEATURE NAMES FROM X\n",
        "\n",
        "# SEÇİLEN ÖZELLİKLERİN İSİMLERİNİ BULMA\n",
        "# get_support(): Seçilen sütunlar için True, seçilmeyenler için False döndürür.\n",
        "# X.columns[...] diyerek sadece True olanların (Seçilenlerin) isimlerini alıyoruz.\n",
        "selected_feature=X.columns[k_best_selector.get_support()]\n",
        "print(\"Seçilen Özellik: \")\n",
        "print(selected_feature)\n",
        "# --- EĞİTİM / TEST AYRIMI ---\n",
        "# Artık elimizdeki veri hem dengelenmiş (augmented) hem de en iyi özellikleri seçilmiş (selected) veridir.\n",
        "X_train,X_test,y_train,y_test=train_test_split(X_selected,y_augmented,test_size=0.2,random_state=42,stratify=y_augmented)\n",
        "\n",
        "models={\n",
        "    \"Logistic Regression\":LogisticRegression(),\n",
        "    \"Decision Tree\":DecisionTreeClassifier(),\n",
        "    \"Random Forest\":RandomForestClassifier(),\n",
        "    \"Gradient Boosting\":GradientBoostingClassifier(),\n",
        "    \"AdaBoost\":AdaBoostClassifier(),\n",
        "    \"SVM\":SVC(),\n",
        "    \"KNN\":KNeighborsClassifier(),\n",
        "    \"Naive Bayes\":GaussianNB()\n",
        "}\n",
        "accuracies=[]\n",
        "for model_name,model in models.items():\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred=model.predict(X_test)\n",
        "  accuracy=accuracy_score(y_test,y_pred)\n",
        "  accuracies.append((model_name,accuracy))\n",
        "  print(f\"{model_name} Accuracy: {accuracy: .4f}\")\n",
        "\n",
        "accuracies_df=pd.DataFrame(accuracies,columns=[\"Model\",\"Accuracy\"]).sort_values(by=\"Accuracy\",ascending=False)\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=\"Accuracy\",y=\"Model\",data=accuracies_df,palette=\"viridis\",hue=\"Model\",legend=False)\n",
        "plt.xlabel(\"Accuracy\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.title(\"Model Doğrulukları\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
